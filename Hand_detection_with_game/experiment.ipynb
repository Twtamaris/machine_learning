{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gives_angle(perp_distance, eyes_distance):\n",
    "    hypotenuse = math.sqrt(perp_distance**2 + eyes_distance**2)\n",
    "    angle = math.atan(perp_distance/eyes_distance)\n",
    "    return angle\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import  math\n",
    "# Load the Haar cascade for eye detection\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "eye_coordinates = []\n",
    "\n",
    "\n",
    "REFERENCE_OBJECT_WIDTH = 20  # Width of the reference object in centimeters\n",
    "FOCAL_LENGTH = 550  # Focal length of the camera in pixels\n",
    "\n",
    "def calculate_distance_to_object(object_width, focal_length, measured_width):\n",
    "    # Calculate the distance to the object using the formula: distance = (reference_width * focal_length) / measured_width\n",
    "    distance = (object_width * focal_length) / measured_width\n",
    "    return distance\n",
    "\n",
    "def calculate_angle_change(x1, y1, x2, y2):\n",
    "    # Calculate the angle between two sets of eye coordinates\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    angle_radians = math.atan2(dy, dx)\n",
    "    angle_degrees = math.degrees(angle_radians)\n",
    "    return angle_degrees\n",
    "\n",
    "\n",
    "screen_width = 1920\n",
    "screen_height = 1080\n",
    "\n",
    "distance = 0\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,10)\n",
    "    for (x,y,w,h) in faces:\n",
    "        distance = calculate_distance_to_object(REFERENCE_OBJECT_WIDTH, FOCAL_LENGTH, w)\n",
    "        cv2.putText(frame, f\"Distance: {distance:.2f} cm\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Gaussian blur to the grayscale frame\n",
    "    blurred_gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    eyes = eye_cascade.detectMultiScale(blurred_gray, scaleFactor=1.3, minNeighbors=10)\n",
    "    eye_coordinates = []\n",
    "    \n",
    "        \n",
    "\n",
    "    if len(eyes) >= 2:\n",
    "\n",
    "        # Iterate over the detected eyes\n",
    "        for (ex, ey, ew, eh) in eyes[:2]:\n",
    "            # Extract the eye region from the frame\n",
    "            eye = frame[ey:ey+eh, ex:ex+ew]\n",
    "\n",
    "            # Convert the eye region to grayscale\n",
    "            eye_gray = cv2.cvtColor(eye, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(eye_gray, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Find contours in the binary image\n",
    "            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Iterate over the contours\n",
    "            for contour in contours:\n",
    "                # Calculate the area of the contour\n",
    "                area = cv2.contourArea(contour)\n",
    "\n",
    "                # Filter contours based on area to find the dark portion of the eyeball\n",
    "                if area > 100:\n",
    "                    # Find the centroid of the contour using the moments\n",
    "                    M = cv2.moments(contour)\n",
    "                    if M[\"m00\"] != 0:\n",
    "                        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "                        # Draw a red dot at the center of the dark portion of the eyeball\n",
    "                        cv2.circle(eye, (cx, cy), 2, (0, 0, 255), -1)\n",
    "                \n",
    "                        \n",
    "\n",
    "\n",
    "            # Append the coordinates of the red dot to the list\n",
    "            eye_coordinates.append([ex+cx, ey+cy])\n",
    "            \n",
    "            # Replace the eye region back in the frame\n",
    "            frame[ey:ey+eh, ex:ex+ew] = eye\n",
    "            \n",
    "            # frame = cv2.circle(frame, (int(ex+ew/2), int(ey+eh/2)), 5, (0, 255, 0), -1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        eye_coordinates = sorted(eye_coordinates, key=lambda x: x[0])\n",
    "\n",
    "        left_eye_center_x = eye_coordinates[0][0]\n",
    "        right_eye_center_x = eye_coordinates[1][0]\n",
    "        left_eye_center_y = eye_coordinates[0][1]\n",
    "        right_eye_center_y = eye_coordinates[1][1]\n",
    "        \n",
    "        # angle = calculate_angle_change(left_eye_center_x, left_eye_center_x, left_eye_center_x, right_eye_center_y)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # print(angle)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "        # print(eye_coordinates)\n",
    "        \n",
    "        \n",
    "        # print(eye_coordinates)\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Constants for reference object size and distance\n",
    "REFERENCE_OBJECT_WIDTH = 20  # Width of the reference object in centimeters\n",
    "FOCAL_LENGTH = 550  # Focal length of the camera in pixels\n",
    "\n",
    "def calculate_distance_to_object(object_width, focal_length, measured_width):\n",
    "    # Calculate the distance to the object using the formula: distance = (reference_width * focal_length) / measured_width\n",
    "    distance = (object_width * focal_length) / measured_width\n",
    "    return distance\n",
    "\n",
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load the face cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    # Read the current frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 10)\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "        # Calculate the distance to the person's face\n",
    "        distance = calculate_distance_to_object(REFERENCE_OBJECT_WIDTH, FOCAL_LENGTH, w)\n",
    "\n",
    "        # Display the distance on the frame\n",
    "        cv2.putText(frame, f\"Distance: {distance:.2f} cm\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Distance Estimation', frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the Haar cascade for eye detection\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    if ret:\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Gaussian blur to the grayscale frame\n",
    "        blurred_gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # Detect eyes in the blurred grayscale frame\n",
    "        eyes = eye_cascade.detectMultiScale(blurred_gray, scaleFactor=1.3, minNeighbors=10)\n",
    "\n",
    "        # Check if both eyes are detected\n",
    "        if len(eyes) == 2:\n",
    "            # Extract the eye centers and sort them based on x-coordinate and y-coordinate\n",
    "            eye_centers = sorted([(ex + ew // 2, ey + eh // 2) for (ex, ey, ew, eh) in eyes])\n",
    "\n",
    "            left_eye_center = eye_centers[0]\n",
    "            right_eye_center = eye_centers[1]\n",
    "\n",
    "            # Compare the relative positions of eye centers to determine the gaze direction\n",
    "            horizontal_gaze = \"Right\" if right_eye_center[0] > left_eye_center[0] else \"Left\"\n",
    "            vertical_gaze = \"Up\" if right_eye_center[1] < left_eye_center[1] else \"Down\"\n",
    "\n",
    "            # Draw circles around the eye centers on the frame\n",
    "            cv2.circle(frame, left_eye_center, 2, (0, 0, 255), -1)\n",
    "            cv2.circle(frame, right_eye_center, 2, (0, 0, 255), -1)\n",
    "\n",
    "            # Display the gaze direction on the frame\n",
    "            cv2.putText(frame, f\"Horizontal Gaze: {horizontal_gaze}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Vertical Gaze: {vertical_gaze}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\baral\\AppData\\Local\\Temp\\ipykernel_11228\\2485610761.py\", line 72, in train_step  *\n        loss = style_content_loss(style_features, content_tensor, style_features_gen)\n    File \"C:\\Users\\baral\\AppData\\Local\\Temp\\ipykernel_11228\\2485610761.py\", line 56, in style_content_loss  *\n        style_loss = tf.add_n([tf.reduce_mean((style_target - tf.image.extract_patches(generated_target, sizes=[1, 3, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')))**2\n\n    ValueError: Dimensions must be equal, but are 224 and 222 for '{{node sub}} = Sub[T=DT_FLOAT](sub/x, ExtractImagePatches)' with input shapes: [1,224,224,64], [1,222,222,576].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\baral\\Desktop\\machine_learning\\Hand_detection_with_game\\experiment.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/baral/Desktop/machine_learning/Hand_detection_with_game/experiment.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m num_iterations \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/baral/Desktop/machine_learning/Hand_detection_with_game/experiment.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/baral/Desktop/machine_learning/Hand_detection_with_game/experiment.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     train_step(generated_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/baral/Desktop/machine_learning/Hand_detection_with_game/experiment.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Convert final generated image tensor to image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/baral/Desktop/machine_learning/Hand_detection_with_game/experiment.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m stylized_image \u001b[39m=\u001b[39m tensor_to_image(generated_image\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\baral\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileuyn2t6a9.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m      9\u001b[0m     style_features_gen \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model), (ag__\u001b[39m.\u001b[39mld(image),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 10\u001b[0m     loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(style_content_loss), (ag__\u001b[39m.\u001b[39mld(style_features), ag__\u001b[39m.\u001b[39mld(content_tensor), ag__\u001b[39m.\u001b[39mld(style_features_gen)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(loss), ag__\u001b[39m.\u001b[39mld(image)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(optimizer)\u001b[39m.\u001b[39mapply_gradients, ([(ag__\u001b[39m.\u001b[39mld(gradients), ag__\u001b[39m.\u001b[39mld(image))],), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7zxie31e.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__style_content_loss\u001b[1;34m(style_targets, content_targets, generated_targets)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m style_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39madd_n, ([ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, (ag__\u001b[39m.\u001b[39mld(style_target) \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches, (ag__\u001b[39m.\u001b[39mld(generated_target),), \u001b[39mdict\u001b[39m(sizes\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m], strides\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], rates\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m'\u001b[39m), fscope),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m (style_target, generated_target) \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(style_targets), ag__\u001b[39m.\u001b[39mld(generated_targets)), \u001b[39mNone\u001b[39;00m, fscope)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m content_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, ((ag__\u001b[39m.\u001b[39mld(content_targets) \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mld(generated_targets)) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m total_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(content_weight) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(content_loss) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(style_weight) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(style_loss)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7zxie31e.py:10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m style_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39madd_n, ([ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, (ag__\u001b[39m.\u001b[39;49mld(style_target) \u001b[39m-\u001b[39;49m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mextract_patches, (ag__\u001b[39m.\u001b[39;49mld(generated_target),), \u001b[39mdict\u001b[39;49m(sizes\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m], strides\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m], rates\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m], padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mVALID\u001b[39;49m\u001b[39m'\u001b[39;49m), fscope),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m (style_target, generated_target) \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(style_targets), ag__\u001b[39m.\u001b[39mld(generated_targets)), \u001b[39mNone\u001b[39;00m, fscope)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m content_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, ((ag__\u001b[39m.\u001b[39mld(content_targets) \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mld(generated_targets)) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m total_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(content_weight) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(content_loss) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(style_weight) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(style_loss)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\baral\\AppData\\Local\\Temp\\ipykernel_11228\\2485610761.py\", line 72, in train_step  *\n        loss = style_content_loss(style_features, content_tensor, style_features_gen)\n    File \"C:\\Users\\baral\\AppData\\Local\\Temp\\ipykernel_11228\\2485610761.py\", line 56, in style_content_loss  *\n        style_loss = tf.add_n([tf.reduce_mean((style_target - tf.image.extract_patches(generated_target, sizes=[1, 3, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')))**2\n\n    ValueError: Dimensions must be equal, but are 224 and 222 for '{{node sub}} = Sub[T=DT_FLOAT](sub/x, ExtractImagePatches)' with input shapes: [1,224,224,64], [1,222,222,576].\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "# Load pre-trained VGG19 model\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# Extract specific layers from VGG19 for style representation\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=vgg.input, outputs=style_outputs)\n",
    "model.trainable = False\n",
    "\n",
    "# Preprocess images by resizing and normalizing pixel values\n",
    "def preprocess_image(image):\n",
    "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    return image\n",
    "\n",
    "# Convert tensor to image\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "# Load content and style images\n",
    "content_image = PIL.Image.open('1.jpg')\n",
    "style_image = PIL.Image.open('2.jpg')\n",
    "\n",
    "# Preprocess content and style images\n",
    "content_array = np.array(content_image)\n",
    "content_array = np.expand_dims(content_array, axis=0)\n",
    "content_tensor = tf.constant(preprocess_image(content_array))\n",
    "\n",
    "style_array = np.array(style_image)\n",
    "style_array = np.expand_dims(style_array, axis=0)\n",
    "style_tensor = tf.constant(preprocess_image(style_array))\n",
    "\n",
    "# Compute style features\n",
    "style_features = model(style_tensor)\n",
    "\n",
    "# Define content and style weights\n",
    "content_weight = 1e3\n",
    "style_weight = 1e-2\n",
    "\n",
    "# Initialize generated image as the content image\n",
    "generated_image = tf.Variable(content_tensor, dtype=tf.float32)\n",
    "\n",
    "# Define loss function for style transfer\n",
    "def style_content_loss(style_targets, content_targets, generated_targets):\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_target - tf.image.extract_patches(generated_target, sizes=[1, 3, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')))**2\n",
    "                           for style_target, generated_target in zip(style_targets, generated_targets)])\n",
    "\n",
    "    content_loss = tf.reduce_mean((content_targets - generated_targets)**2)\n",
    "\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    return total_loss\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "# Perform style transfer iterations\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        style_features_gen = model(image)\n",
    "        loss = style_content_loss(style_features, content_tensor, style_features_gen)\n",
    "    gradients = tape.gradient(loss, image)\n",
    "    optimizer.apply_gradients([(gradients, image)])\n",
    "    image.assign(tf.clip_by_value(image, clip_value_min=-1.0, clip_value_max=1.0))\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    train_step(generated_image)\n",
    "\n",
    "# Convert final generated image tensor to image\n",
    "stylized_image = tensor_to_image(generated_image.numpy())\n",
    "\n",
    "# Save the stylized image\n",
    "stylized_image.save('stylized_image.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
